use serde::Serialize;
use std::fs;
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;
use tauri::{AppHandle, Manager, State};
use time::format_description::well_known::Rfc3339;
use time::macros::format_description;
use time::OffsetDateTime;

use crate::history::{HistoryState, ensure_daily_file};
use crate::llm;
use crate::pii_masker;
use crate::utils::*;

/// AI í”¼ë“œë°± ìŠ¤íŠ¸ë¦¬ë° ì·¨ì†Œ ìƒíƒœ ê´€ë¦¬
pub struct StreamCancellationState {
    pub is_cancelled: Arc<AtomicBool>,
}

impl Default for StreamCancellationState {
    fn default() -> Self {
        Self {
            is_cancelled: Arc::new(AtomicBool::new(false)),
        }
    }
}

impl StreamCancellationState {
    pub fn reset(&self) {
        self.is_cancelled.store(false, Ordering::SeqCst);
    }

    pub fn cancel(&self) {
        self.is_cancelled.store(true, Ordering::SeqCst);
    }

    pub fn is_cancelled(&self) -> bool {
        self.is_cancelled.load(Ordering::SeqCst)
    }
}

#[derive(Debug, Serialize, Clone)]
#[serde(rename_all = "camelCase")]
pub struct AiSummaryFile {
    pub filename: String,
    pub path: String,
    pub created_at: Option<String>,
    pub content: String,
    pub pii_masked: bool, // ê°œì¸ì •ë³´ ë³´í˜¸ ì—¬ë¶€
}

/// AI ìš”ì•½ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
pub fn ensure_summaries_dir(path: &PathBuf) -> Result<(), String> {
    fs::create_dir_all(path)
        .map_err(|error| format!("AI ìš”ì•½ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {error}, ê²½ë¡œ: {:?}", path))
}

/// AI ìš”ì•½ íŒŒì¼ì„ ì‘ì„±í•©ë‹ˆë‹¤
fn write_ai_summary_file(date: &OffsetDateTime, content: &str, pii_masked: bool) -> Result<AiSummaryFile, String> {
    let dir = summaries_directory_path()?;
    ensure_summaries_dir(&dir)?;

    let date_key = date
        .format(&format_description!(
            "[year][month][day]-[hour][minute][second]"
        ))
        .map_err(|error| error.to_string())?;

    let mut filename = format!("ai-feedback-{date_key}.md");
    let mut path = dir.join(&filename);
    let mut suffix = 1;

    while path.exists() {
        filename = format!("ai-feedback-{date_key}-{suffix}.md");
        path = dir.join(&filename);
        suffix += 1;
    }

    fs::write(&path, content.as_bytes()).map_err(|error| format!("AI ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {error}"))?;

    let created_at = date.format(&Rfc3339).ok();

    Ok(AiSummaryFile {
        filename,
        path: path.to_string_lossy().into_owned(),
        created_at,
        content: content.to_string(),
        pii_masked,
    })
}

// Tauri Commands

#[tauri::command]
pub async fn generate_ai_feedback(
    history: State<'_, HistoryState>,
    llm_state: tauri::State<'_, Arc<llm::LLMManager>>,
    model_selection_state: State<'_, crate::model_selection::ModelSelectionState>,
) -> Result<AiSummaryFile, String> {
    let now = current_local_time()?;
    let (today_path, _) = ensure_daily_file(history.inner(), &now)?;
    let today_content = fs::read_to_string(&today_path).unwrap_or_default();

    if today_content.trim().is_empty() {
        return Err("ì˜¤ëŠ˜ ê¸°ë¡ëœ ë‚´ìš©ì´ ì—†ì–´ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.".into());
    }

    // ì„ íƒëœ ëª¨ë¸ í™•ì¸
    let selected_model_lock = model_selection_state.selected.read().await;
    let selected_model = selected_model_lock.clone();
    drop(selected_model_lock);

    // ëª¨ë¸ íƒ€ì… ê²°ì •
    let use_cloud_llm = if let Some(ref model) = selected_model {
        model.model_type == "cloud"
    } else {
        // ì„ íƒëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©
        false
    };

    // ğŸ”’ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ (í´ë¼ìš°ë“œ LLM ì‚¬ìš© ì‹œì—ë§Œ)
    let (masked_content, pii_detected) = if use_cloud_llm {
        eprintln!("[PII Masking] Cloud LLM detected - applying PII masking");
        let masked = pii_masker::mask_pii(&today_content, false);
        let detected = today_content != masked;

        eprintln!("[AI Feedback] Original length: {} chars", today_content.len());
        eprintln!("[AI Feedback] Masked length: {} chars", masked.len());
        if detected {
            eprintln!("[PII Masking] âš ï¸ PII detected and masked");
        } else {
            eprintln!("[PII Masking] âœ… No PII detected");
        }

        (masked, detected)
    } else {
        eprintln!("[PII Masking] Local model detected - skipping PII masking");
        (today_content.clone(), false)
    };

    // ê¸¸ì´ ì¡°ì •: ì½”ì¹˜í˜• í”¼ë“œë°±(Paragraph)ë¡œ 500ë‹¨ì–´ ë‚´ì™¸ ìš”ì²­ â†’ ì¶©ë¶„í•œ ë°€ë„ì˜ ê²°ê³¼
    let request = llm::summarize::SummaryRequest {
        content: masked_content,
        style: None, // í”„ë¡¬í”„íŠ¸ëŠ” use_local_promptë¡œ ê²°ì •ë¨
        max_length: Some(500),
        model_id: None,
        use_local_prompt: Some(!use_cloud_llm), // ë¡œì»¬ ëª¨ë¸ì´ë©´ true, í´ë¼ìš°ë“œë©´ false
    };

    let summary = match tokio::time::timeout(
        std::time::Duration::from_secs(90),
        llm::summarize::summarize_note(llm_state, request),
    )
    .await
    {
        Ok(result) => result?,
        Err(_) => return Err("ìš”ì•½ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤ (90ì´ˆ)".into()),
    };

    let summary_body = summary.summary.trim();

    // LLMì´ ìƒì„±í•œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë©”íƒ€ í—¤ë” ì œê±°)
    let markdown = if summary_body.is_empty() {
        "(ìƒì„±ëœ ìš”ì•½ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤)".to_string()
    } else {
        summary_body.to_string()
    };

    write_ai_summary_file(&now, &markdown, pii_detected)
}

#[tauri::command]
pub async fn generate_ai_feedback_stream(
    app: AppHandle,
    history: State<'_, HistoryState>,
    llm_state: tauri::State<'_, Arc<llm::LLMManager>>,
    cloud_llm_state: State<'_, llm::CloudLLMState>,
    model_selection_state: State<'_, crate::model_selection::ModelSelectionState>,
    cancellation_state: State<'_, StreamCancellationState>,
    target_date: Option<String>, // Optional target date in YYYY-MM-DD format
) -> Result<(), String> {
    // ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì‹œ ì·¨ì†Œ í”Œë˜ê·¸ ì´ˆê¸°í™”
    cancellation_state.reset();
    // Determine the target date
    let target_time = if let Some(date_str) = target_date {
        // Parse the provided date (YYYY-MM-DD format)
        let date_format = time::format_description::parse("[year]-[month]-[day]")
            .map_err(|e| format!("ë‚ ì§œ í˜•ì‹ íŒŒì‹± ì‹¤íŒ¨: {}", e))?;

        let date = time::Date::parse(&date_str, &date_format)
            .map_err(|e| format!("ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({}): {}", date_str, e))?;

        // Convert to OffsetDateTime with current local time zone
        let current_offset = OffsetDateTime::now_local()
            .map_err(|e| format!("ë¡œì»¬ ì‹œê°„ ì˜¤í”„ì…‹ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {}", e))?
            .offset();

        date.with_hms(12, 0, 0)
            .map_err(|e| format!("ì‹œê°„ ì„¤ì • ì‹¤íŒ¨: {}", e))?
            .assume_offset(current_offset)
    } else {
        // Use today's date
        current_local_time()?
    };

    let (target_path, _) = ensure_daily_file(history.inner(), &target_time)?;
    let today_content = fs::read_to_string(&target_path).unwrap_or_default();

    if today_content.trim().is_empty() {
        return Err("ì˜¤ëŠ˜ ê¸°ë¡ëœ ë‚´ìš©ì´ ì—†ì–´ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.".into());
    }

    // ì„ íƒëœ ëª¨ë¸ í™•ì¸
    let selected_model_lock = model_selection_state.selected.read().await;
    let selected_model = selected_model_lock.clone();
    drop(selected_model_lock);

    // ëª¨ë¸ íƒ€ì… ê²°ì •
    let use_cloud_llm = if let Some(ref model) = selected_model {
        model.model_type == "cloud"
    } else {
        // ì„ íƒëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¡œì»¬ ëª¨ë¸ í™•ì¸
        let engine = llm_state.engine.lock().await;
        !engine.is_running()
    };

    // ğŸ”’ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ (í´ë¼ìš°ë“œ LLM ì‚¬ìš© ì‹œì—ë§Œ)
    let (masked_content, pii_detected) = if use_cloud_llm {
        eprintln!("[PII Masking] Cloud LLM detected - applying PII masking");
        let masked = pii_masker::mask_pii(&today_content, false);
        let detected = today_content != masked;

        eprintln!("[PII Masking] Original length: {} chars", today_content.len());
        eprintln!("[PII Masking] Masked length: {} chars", masked.len());
        if detected {
            eprintln!("[PII Masking] âš ï¸ PII detected and masked");
        } else {
            eprintln!("[PII Masking] âœ… No PII detected");
        }

        (masked, detected)
    } else {
        eprintln!("[PII Masking] Local model detected - skipping PII masking");
        (today_content.clone(), false)
    };

    // ë§ˆìŠ¤í‚¹ í†µê³„ë¥¼ í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ì†¡ (ê°œë°œ ëª¨ë“œ ê²€ì¦ìš©)
    if let Err(e) = app.emit_all(
        "ai_feedback_masking_stats",
        serde_json::json!({
            "originalLength": today_content.len(),
            "maskedLength": masked_content.len(),
            "piiDetected": pii_detected,
        }),
    ) {
        eprintln!("[PII Masking] Failed to emit masking stats: {}", e);
    }

    // í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì„ íƒ)
    let prompt = if use_cloud_llm {
        eprintln!("[Prompt Selection] Using cloud prompt (deep cognitive analysis)");
        llm::prompts::PromptTemplate::for_business_journal_coach(&masked_content)
    } else {
        eprintln!("[Prompt Selection] Using local prompt (simplified 3-section)");
        llm::prompts::PromptTemplate::for_local_model(&masked_content)
    };
    let chat_messages = prompt.to_chat_format();

    // ëª¨ë¸ë³„ ì²˜ë¦¬ ë° ê²°ê³¼ ë°˜í™˜
    let result = if use_cloud_llm {
        // Cloud LLM ì‚¬ìš©
        eprintln!("[AI Feedback] Using Cloud LLM");

        // ì„ íƒëœ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        let cloud_model_id = selected_model
            .as_ref()
            .map(|m| m.model_id.clone())
            .unwrap_or_else(|| "gpt-4-turbo".to_string());

        eprintln!("[AI Feedback] Selected cloud model: {}", cloud_model_id);

        // í”„ë¡¬í”„íŠ¸ë¥¼ Cloud LLM í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        let messages: Vec<llm::types::Message> = chat_messages
            .iter()
            .map(|msg| llm::types::Message {
                role: match msg.role.as_str() {
                    "system" => llm::types::Role::System,
                    "user" => llm::types::Role::User,
                    "assistant" => llm::types::Role::Assistant,
                    _ => llm::types::Role::User,
                },
                content: msg.content.clone(),
            })
            .collect();

        let request = llm::types::CompletionRequest {
            messages,
            model: cloud_model_id.clone(),
            temperature: Some(0.7),
            max_tokens: Some(4000),
            system_prompt: None,
            metadata: None,
        };

        // CloudLLM provider ê°€ì ¸ì˜¤ê¸°
        let provider_lock = cloud_llm_state.current_provider.read().await;

        if let Some(provider) = provider_lock.as_ref() {
            // ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ
            match provider.stream(request).await {
                Ok(mut rx) => {
                    let mut full_text = String::new();
                    let emit_handle = app.clone();
                    let mut cancelled = false;

                    // ìŠ¤íŠ¸ë¦¼ì—ì„œ ë¸íƒ€ ìˆ˜ì‹ í•˜ë©° emit
                    while let Some(delta) = rx.recv().await {
                        // ì·¨ì†Œ í™•ì¸
                        if cancellation_state.is_cancelled() {
                            eprintln!("[Cloud LLM Stream] Cancelled by user");
                            cancelled = true;
                            break;
                        }

                        full_text.push_str(&delta);

                        // ë¸íƒ€ë¥¼ í”„ë¡ íŠ¸ì—”ë“œë¡œ emit
                        if let Err(e) = emit_handle.emit_all(
                            "ai_feedback_stream_delta",
                            &serde_json::json!({ "text": delta }),
                        ) {
                            eprintln!("[Cloud LLM Stream] emit delta failed: {}", e);
                            break;
                        }
                    }

                    // ì·¨ì†ŒëŠ” ì—ëŸ¬ê°€ ì•„ë‹ˆë¼ ì¡°ê¸° ì¢…ë£Œë¡œ ì²˜ë¦¬
                    if cancelled {
                        // ì·¨ì†Œëœ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ê³  í•¨ìˆ˜ë¥¼ ì¡°ê¸° ë°˜í™˜
                        return Ok(());
                    }

                    Ok(full_text)
                }
                Err(e) => {
                    Err(format!("Cloud LLM ì˜¤ë¥˜: {}", e))
                }
            }
        } else {
            Err("Cloud LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.".to_string())
        }
    } else {
        // ë¡œì»¬ LLM ì‚¬ìš©
        let mut engine = llm_state.engine.lock().await;

        let mut last_emit_ok = true;
        let emit_handle = app.clone();
        let cancel_check = cancellation_state.clone();

        // ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        let result = engine
            .chat_complete_stream(
                chat_messages,
                None,
                None,
                |delta| {
                    // ì·¨ì†Œ í™•ì¸
                    if cancel_check.is_cancelled() {
                        return;
                    }

                    if last_emit_ok {
                        if let Err(e) = emit_handle.emit_all(
                            "ai_feedback_stream_delta",
                            &serde_json::json!({ "text": delta }),
                        ) {
                            eprintln!("[AI Stream] emit delta failed: {}", e);
                            last_emit_ok = false;
                        }
                    }
                },
            )
            .await
            .map_err(|e| e.to_string());

        // ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
        if cancellation_state.is_cancelled() {
            // ì·¨ì†Œëœ ê²½ìš° ì •ìƒ ì¢…ë£Œ
            return Ok(());
        }

        result
    };

    match result {
        Ok(full_text) => {
            // LLMì´ ìƒì„±í•œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë©”íƒ€ í—¤ë” ì œê±°)
            let markdown = if full_text.trim().is_empty() {
                "(ìƒì„±ëœ ìš”ì•½ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤)".to_string()
            } else {
                full_text.trim().to_string()
            };

            match write_ai_summary_file(&target_time, &markdown, pii_detected) {
                Ok(saved) => {
                    let _ = app.emit_all(
                        "ai_feedback_stream_complete",
                        &serde_json::json!({
                            "filename": saved.filename,
                            "path": saved.path,
                            "createdAt": saved.created_at,
                        }),
                    );
                    Ok(())
                }
                Err(e) => {
                    let _ = app.emit_all(
                        "ai_feedback_stream_error",
                        &serde_json::json!({ "message": e }),
                    );
                    Err(e)
                }
            }
        }
        Err(e) => {
            let msg = e.to_string();
            let _ = app.emit_all(
                "ai_feedback_stream_error",
                &serde_json::json!({ "message": msg }),
            );
            Err(msg)
        }
    }
}

/// AI í”¼ë“œë°± ìŠ¤íŠ¸ë¦¬ë°ì„ ì·¨ì†Œí•©ë‹ˆë‹¤
#[tauri::command]
pub async fn cancel_ai_feedback_stream(
    app: AppHandle,
    cancellation_state: State<'_, StreamCancellationState>,
) -> Result<(), String> {
    eprintln!("[AI Feedback] Cancel requested");
    cancellation_state.cancel();

    // ì·¨ì†Œ ì´ë²¤íŠ¸ë¥¼ í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ì†¡
    if let Err(e) = app.emit_all(
        "ai_feedback_stream_cancelled",
        &serde_json::json!({}),
    ) {
        eprintln!("[AI Feedback] Failed to emit cancellation event: {}", e);
    }

    Ok(())
}

#[tauri::command]
pub fn list_ai_summaries(limit: Option<usize>, target_date: Option<String>) -> Result<Vec<AiSummaryFile>, String> {
    let dir = summaries_directory_path()?;
    ensure_summaries_dir(&dir)?;

    // Determine which date to filter for
    let date_key = if let Some(date_str) = target_date {
        // Parse YYYY-MM-DD format and convert to YYYYMMDD
        let date_format = time::format_description::parse("[year]-[month]-[day]")
            .map_err(|e| format!("ë‚ ì§œ í˜•ì‹ íŒŒì‹± ì‹¤íŒ¨: {}", e))?;

        let date = time::Date::parse(&date_str, &date_format)
            .map_err(|e| format!("ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({}): {}", date_str, e))?;

        // Convert to YYYYMMDD format
        date.format(&time::macros::format_description!("[year][month][day]"))
            .map_err(|e| format!("ë‚ ì§œ í¬ë§· ì‹¤íŒ¨: {}", e))?
    } else {
        // Use today's date
        let today = current_local_time()?;
        format_date_key(&today)?
    };

    let mut summaries: Vec<(OffsetDateTime, AiSummaryFile)> = fs::read_dir(&dir)
        .map_err(|error| error.to_string())?
        .filter_map(|entry| match entry {
            Ok(entry) => {
                let path = entry.path();
                if path.extension().and_then(|ext| ext.to_str()) != Some("md") {
                    return None;
                }
                let filename = entry.file_name().to_string_lossy().into_owned();

                // Filter for the specified date's summaries (ai-feedback-YYYYMMDD*.md pattern)
                if !filename.starts_with(&format!("ai-feedback-{date_key}")) {
                    return None;
                }

                let content = fs::read_to_string(&path).unwrap_or_default();

                // ê°œì¸ì •ë³´ ë³´í˜¸ ë©”íƒ€ë°ì´í„° íŒŒì‹±
                let pii_masked = content.contains("ê°œì¸ì •ë³´ ë³´í˜¸: ì ìš©ë¨");

                let metadata = entry.metadata().ok();
                let (sort_key, created_at) = metadata
                    .and_then(|meta| meta.modified().ok())
                    .map(|modified| {
                        let odt: OffsetDateTime = modified.into();
                        let iso = odt.format(&Rfc3339).unwrap_or_else(|_| odt.to_string());
                        (odt, Some(iso))
                    })
                    .unwrap_or((OffsetDateTime::UNIX_EPOCH, None));

                Some((
                    sort_key,
                    AiSummaryFile {
                        filename,
                        path: path.to_string_lossy().into_owned(),
                        created_at,
                        content,
                        pii_masked,
                    },
                ))
            }
            Err(_) => None,
        })
        .collect();

    summaries.sort_by(|a, b| b.0.cmp(&a.0));
    let limit = limit.unwrap_or(10);
    Ok(summaries
        .into_iter()
        .take(limit)
        .map(|(_, summary)| summary)
        .collect())
}
